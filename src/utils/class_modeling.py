from typing import List, Dict, Optional, Union
import pandas as pd
import numpy as np
from time import time
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from lightgbm import LGBMClassifier
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import (
    f1_score,
    roc_curve,
    auc,
    precision_recall_curve,
    average_precision_score,
    classification_report, 
    accuracy_score, 
    roc_auc_score, 
    confusion_matrix,
    precision_score, 
    recall_score, 
)
import plotly.graph_objects as go
from IPython.display import display, HTML
import matplotlib.pyplot as plt
import seaborn as sns

class ModelComparator:
    """
    Classe pour comparer plusieurs mod√®les de classification avec optimisation bay√©sienne,
    visualiser leurs performances (ROC, PR) et identifier le meilleur seuil F1-score.
    """

    def __init__(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_test: pd.DataFrame,
        y_test: pd.Series,
        random_state: int = 0,
        list_models: Optional[List[str]] = None
    ):
        """
        Initialise la classe avec les jeux de donn√©es.

        Parameters
        ----------
        X_train : pd.DataFrame
        y_train : pd.Series
        X_test  : pd.DataFrame
        y_test  : pd.Series
        random_state : int
            Graine de reproductibilit√©
        list_models : list, optional
            Liste des mod√®les √† tester (ex : ["Random Forest", "Logistic Regression"]).
        """
        self.X_train = X_train
        self.y_train = y_train
        self.X_test = X_test
        self.y_test = y_test
        self.random_state = random_state
        self.scoring = "f1"
        self.list_models = list_models
        self.opt_by_model: Dict[str, BayesSearchCV] = {}
        self.proba_dict: Dict[str, np.ndarray] = {}
        self.df_resultats: pd.DataFrame = pd.DataFrame()

        # Obtenir tous les mod√®les disponibles
        self.all_model_spaces = self._get_model_spaces()

        # Expansion des mod√®les "g√©n√©raux"
        if list_models:
            expanded = []
            for model in list_models:
                if model == "Logistic Regression":
                    expanded.extend([
                        "Logistic Regression (L1)",
                        "Logistic Regression (L2)",
                        "Logistic Regression (ElasticNet)"
                    ])
                else:
                    expanded.append(model)

            # V√©rification validit√©
            unknown = [m for m in expanded if m not in self.all_model_spaces]
            if unknown:
                raise ValueError(f"Mod√®le(s) inconnu(s) : {unknown}\nMod√®les disponibles : {list(self.all_model_spaces.keys())}")
            self.selected_models = {k: v for k, v in self.all_model_spaces.items() if k in expanded}
        else:
            self.selected_models = self.all_model_spaces

    def _get_model_spaces(self) -> Dict[str, tuple]:
        """
        D√©finit les mod√®les et leur espace d'hyperparam√®tres.
        """
        common_tree_params = {
            'n_estimators': Integer(100, 300),
            'max_depth': Integer(3, 15),
            'min_samples_leaf': Integer(1, 10),
            'max_features': Categorical(['sqrt', 'log2']),
        }

        return {
            "Random Forest": (
                RandomForestClassifier(random_state=self.random_state, verbose=0),
                {**common_tree_params, 'bootstrap': Categorical([True])}
            ),
            "Extra Trees": (
                ExtraTreesClassifier(random_state=self.random_state, verbose=0),
                {**common_tree_params, 'bootstrap': Categorical([True])}
            ),
            "Gradient Boosting": (
                GradientBoostingClassifier(random_state=self.random_state, verbose=0),
                {**common_tree_params, 'learning_rate': Real(0.01, 0.3, prior='log-uniform')}
            ),
            "LightGBM": (
                LGBMClassifier(random_state=self.random_state, verbose=0),
                {
                    'n_estimators': Integer(100, 300),
                    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),
                    'max_depth': Integer(3, 15),
                    'num_leaves': Integer(15, 100),
                    'min_child_samples': Integer(5, 20)
                }
            ),
            "Logistic Regression (L1)": (
                LogisticRegression(max_iter=1000, random_state=self.random_state),
                {
                    'penalty': Categorical(['l1']),
                    'solver': Categorical(['liblinear', 'saga']),
                    'C': Real(0.001, 10, prior='log-uniform'),
                    'tol': Real(1e-4, 1e-2, prior='log-uniform')
                }
            ),
            "Logistic Regression (L2)": (
                LogisticRegression(max_iter=1000, random_state=self.random_state),
                {
                    'penalty': Categorical(['l2']),
                    'solver': Categorical(['newton-cg', 'lbfgs', 'sag', 'saga']),
                    'C': Real(0.001, 10, prior='log-uniform'),
                    'tol': Real(1e-4, 1e-2, prior='log-uniform')
                }
            ),
            "Logistic Regression (ElasticNet)": (
                LogisticRegression(max_iter=1000, random_state=self.random_state),
                {
                    'penalty': Categorical(['elasticnet']),
                    'solver': Categorical(['saga']),
                    'C': Real(0.001, 10, prior='log-uniform'),
                    'l1_ratio': Real(0.1, 0.9, prior='uniform'),
                    'tol': Real(1e-4, 1e-2, prior='log-uniform')
                }
            )
        }

    def get_proba_dict(self) -> Dict[str, np.ndarray]:
        """
        Calcule les probabilit√©s pr√©dites sur X_test pour chaque mod√®le.
        """
        self.proba_dict = {
            name: opt.best_estimator_.predict_proba(self.X_test)[:, 1]
            for name, opt in self.opt_by_model.items()
        }
        return self.proba_dict
    

    def bayes_optimize_models(
        self,
        n_iter: int = 50,
        cv_folds: int = 5,
        scoring: str = "f1"
    ) -> pd.DataFrame:
        """
        Effectue l‚Äôoptimisation bay√©sienne sur une liste de mod√®les.

        Returns
        -------
        pd.DataFrame :
            R√©sum√© des performances (score CV + dur√©e).
        """
        self.scoring = scoring
        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)
        models = self._get_model_spaces()

        if self.list_models:
            expanded = []
            for model in self.list_models:
                if model == "Logistic Regression":
                    expanded += [
                        "Logistic Regression (L1)",
                        "Logistic Regression (L2)",
                        "Logistic Regression (ElasticNet)"
                    ]
                else:
                    expanded.append(model)
            models = {k: v for k, v in models.items() if k in expanded}

        results = []
        for name, (estimator, space) in models.items():
            print(f"üîç Optimisation en cours : {name}")
            start = time()
            opt = BayesSearchCV(
                estimator=estimator,
                search_spaces=space,
                n_iter=n_iter,
                scoring=scoring,
                cv=cv,
                n_jobs=-1,
                verbose=0,
                random_state=self.random_state
            )
            opt.fit(self.X_train, self.y_train)
            duration = round(time() - start, 2)

            results.append({
                "Mod√®le": name,
                f"Score {scoring.upper()} (CV)": round(opt.best_score_, 8),
                "Dur√©e (s)": duration
            })
            self.opt_by_model[name] = opt

        self.df_resultats = pd.DataFrame(results).sort_values(by=f"Score {scoring.upper()} (CV)", ascending=False)
        self.proba_dict = self.get_proba_dict()
        return self.df_resultats

    def get_best_model(self) -> Dict[str, Union[str, float, object]]:
        """
        Retourne le meilleur mod√®le selon la m√©trique d'√©valuation (scoring).

        Returns
        -------
        dict :
            {
                "model_name" : str ‚Üí nom du meilleur mod√®le,
                "score"      : float ‚Üí score CV associ√©,
                "estimator"  : object ‚Üí objet sklearn entra√Æn√© (best_estimator_)
            }
        """
        if self.df_resultats.empty:
            raise ValueError("Aucun r√©sultat disponible. Veuillez ex√©cuter bayes_optimize_models() d'abord.")

        # Nom de la colonne de score (ex: "Score F1 (CV)")
        score_col = [col for col in self.df_resultats.columns if "Score" in col][0]
        best_row = self.df_resultats.loc[self.df_resultats[score_col].idxmax()]

        model_name = best_row["Mod√®le"]
        best_score = best_row[score_col]
        best_estimator = self.opt_by_model[model_name].best_estimator_

        print(f"Meilleur mod√®le : {model_name}")
        print(f"Score ({self.scoring.upper()} CV) : {best_score:.4f}")

        return {
            "model_name": model_name,
            "score": best_score,
            "estimator": best_estimator,
            "y_proba": self.proba_dict[model_name]
        }

    def plot_roc_curve_interactive(self) -> None:
        """
        Affiche les courbes ROC tri√©es par AUC.
        """
        fig = go.Figure()
        data = []

        for model_name, y_prob in self.proba_dict.items():
            fpr, tpr, _ = roc_curve(self.y_test, y_prob)
            auc_score = auc(fpr, tpr)
            data.append((model_name, fpr, tpr, auc_score))

        for model_name, fpr, tpr, auc_score in sorted(data, key=lambda x: x[3], reverse=True):
            fig.add_trace(go.Scatter(
                x=fpr, y=tpr, mode='lines',
                name=f"{model_name} (AUC = {auc_score:.3f})",
                hovertemplate="FPR = %{x:.2f}<br>TPR = %{y:.2f}<extra></extra>"
            ))

        fig.add_trace(go.Scatter(
            x=[0, 1], y=[0, 1],
            mode='lines',
            name="Mod√®le al√©atoire",
            line=dict(dash='dash', color='gray'),
            hoverinfo='skip'
        ))

        fig.update_layout(
            title="üìä Courbes ROC compar√©es",
            xaxis_title="Taux de faux positifs (FPR)",
            yaxis_title="Taux de vrais positifs (TPR)",
            template="plotly_white",
            width=850,
            height=520
        )
        fig.show()

    def plot_all_bayes_convergences(self) -> None:
        """
        Affiche un graphique interactif Plotly avec les courbes de convergence
        de tous les mod√®les optimis√©s avec BayesSearchCV.

        Cela permet de visualiser l‚Äô√©volution du score (ex : F1, AUC...) √† chaque it√©ration
        pour comparer la stabilit√© et la vitesse de convergence de chaque mod√®le.
        """
        if not self.opt_by_model:
            print("‚ùå Aucun mod√®le n'a √©t√© optimis√©. Lancez compare_models() d'abord.")
            return

        fig = go.Figure()

        for model_name, opt in self.opt_by_model.items():
            scores = opt.cv_results_["mean_test_score"]
            iterations = list(range(1, len(scores) + 1))

            best_iter = int(np.argmax(scores)) + 1
            best_score = np.max(scores)

            # Trace principale
            fig.add_trace(go.Scatter(
                x=iterations,
                y=scores,
                mode='lines+markers',
                name=f"{model_name} (max={best_score:.4f})",
                hovertemplate="It√©ration = %{x}<br>Score = %{y:.4f}<extra></extra>"
            ))

            # Marqueur du meilleur score
            fig.add_trace(go.Scatter(
                x=[best_iter],
                y=[best_score],
                mode="markers",
                name=f"{model_name} ‚òÖ",
                marker=dict(color='red', size=8, symbol='star'),
                showlegend=False,
                hovertemplate=f"{model_name}<br>It√©ration optimale = {best_iter}<br>Score = {best_score:.4f}<extra></extra>"
            ))

        # Label de score
        scoring = list(self.opt_by_model.values())[0].scoring
        score_label = {
            'roc_auc': 'ROC AUC',
            'f1': 'F1-score',
            'accuracy': 'Accuracy',
            'precision': 'Pr√©cision',
            'recall': 'Recall'
        }.get(scoring, scoring.upper())

        fig.update_layout(
            title=f"üìâ Courbes de convergence Bay√©sienne ({score_label})",
            xaxis_title="It√©rations",
            yaxis_title=score_label,
            template="plotly_white",
            width=950,
            height=550,
            legend_title="Mod√®les"
        )

        fig.show()


    def plot_precision_recall_curve_interactive(self) -> None:
        """
        Affiche les courbes Pr√©cision-Rappel tri√©es par Average Precision.
        """
        fig = go.Figure()
        data = []

        for model_name, y_prob in self.proba_dict.items():
            precision, recall, _ = precision_recall_curve(self.y_test, y_prob)
            ap = average_precision_score(self.y_test, y_prob)
            data.append((model_name, precision, recall, ap))

        for model_name, precision, recall, ap in sorted(data, key=lambda x: x[3], reverse=True):
            fig.add_trace(go.Scatter(
                x=recall, y=precision, mode='lines',
                name=f"{model_name} (AP = {ap:.3f})",
                hovertemplate="Recall = %{x:.2f}<br>Pr√©cision = %{y:.2f}<extra></extra>"
            ))

        fig.update_layout(
            title="üìà Courbes Pr√©cision-Rappel compar√©es",
            xaxis_title="Recall",
            yaxis_title="Precision",
            template="plotly_white",
            width=850,
            height=520
        )
        fig.show()

    def best_f1_by_model(self) -> Dict[str, Dict[str, Union[float, np.ndarray]]]:
        """
        Calcule le seuil optimal qui maximise le F1-score pour chaque mod√®le.

        Returns
        -------
        dict :
            {
                "f1_scores": {mod√®le: f1_max},
                "opti_thresholds": {mod√®le: seuil_optimal}
            }
        """
        seuils = {}
        f1_scores = {}
        curves_data = []

        for model_name, y_prob in self.proba_dict.items():
            thresholds = np.linspace(0.01, 0.99, 100)
            f1_list = [f1_score(self.y_test, (y_prob >= t).astype(int)) for t in thresholds]
            idx_best = int(np.argmax(f1_list))
            seuils[model_name] = thresholds[idx_best]
            f1_scores[model_name] = f1_list[idx_best]

            curves_data.append({
                "model": model_name,
                "thresholds": thresholds,
                "f1_scores": f1_list,
                "f1_max": f1_list[idx_best]
            })

        fig = go.Figure()
        for data in sorted(curves_data, key=lambda x: x["f1_max"], reverse=True):
            fig.add_trace(go.Scatter(
                x=data["thresholds"],
                y=data["f1_scores"],
                mode="lines+markers",
                name=f"{data['model']} (max={data['f1_max']:.3f})",
                hovertemplate="Seuil = %{x:.2f}<br>F1-score = %{y:.3f}<extra></extra>"
            ))

        fig.update_layout(
            title="üéØ F1-score en fonction du seuil (tri√© par performance)",
            xaxis_title="Seuil de d√©cision",
            yaxis_title="F1-score",
            template="plotly_white",
            width=950,
            height=520,
            yaxis=dict(range=[0, 1.02])
        )
        fig.show()

        return {
            "f1_scores": f1_scores,
            "opti_thresholds": seuils
        }

    def evaluate_model(self, model_name: str, threshold: Optional[float] = None) -> None:
        """
        Affiche un rapport HTML interpr√©table pour le mod√®le s√©lectionn√©.

        Parameters
        ----------
        model_name : str
            Le nom du mod√®le tel qu'il appara√Æt dans self.proba_dict.
        threshold : float, optional
            Seuil √† utiliser pour binariser les probabilit√©s (d√©faut = 0.5).
        """
        if model_name not in self.proba_dict:
            print(f"‚ùå Mod√®le {model_name} introuvable.")
            return

        y_prob = self.proba_dict[model_name]
        y_pred = (y_prob >= (threshold if threshold else 0.5)).astype(int)

        # M√©triques
        acc = accuracy_score(self.y_test, y_pred)
        auc_score = roc_auc_score(self.y_test, y_prob)
        report = classification_report(self.y_test, y_pred, output_dict=True, target_names=["Non d√©faut", "D√©faut"])

        # Construction tableau HTML
        html = f"""
        <div class="alert alert-success" style="font-family:Arial;">
        <h4>üìä √âvaluation du mod√®le : <code>{model_name}</code></h4>

        <h5>‚öôÔ∏è M√©triques globales</h5>
        <table border="1" style="border-collapse:collapse; width:100%; text-align:left;">
            <thead style="background-color:#f2f2f2;">
            <tr><th style="padding:6px;">M√©trique</th><th style="padding:6px;">Valeur</th></tr>
            </thead>
            <tbody>
            <tr>
                <td style="padding:6px;">Accuracy</td>
                <td style="padding:6px;">{acc:.3f}</td>

            </tr>
            <tr>
                <td style="padding:6px;">ROC AUC</td>
                <td style="padding:6px;">{auc_score:.3f}</td>
            </tr>
            </tbody>
        </table>

        <h5 style="margin-top:20px;">üéØ M√©triques par classe</h5>
        <table border="1" style="border-collapse:collapse; width:100%; text-align:left;">
            <thead style="background-color:#f2f2f2;">
            <tr>
                <th style="padding:6px;">Classe</th>
                <th style="padding:6px;">Pr√©cision</th>
                <th style="padding:6px;">Rappel</th>
                <th style="padding:6px;">F1-score</th>
            </tr>
            </thead>
            <tbody>
            <tr>
                <td style="padding:6px;"><b>Non d√©faut</b></td>
                <td style="padding:6px;">{report['Non d√©faut']['precision']:.3f}</td>
                <td style="padding:6px;">{report['Non d√©faut']['recall']:.3f}</td>
                <td style="padding:6px;">{report['Non d√©faut']['f1-score']:.3f}</td>
            </tr>
            <tr>
                <td style="padding:6px;"><b>D√©faut</b></td>
                <td style="padding:6px;">{report['D√©faut']['precision']:.3f}</td>
                <td style="padding:6px;">{report['D√©faut']['recall']:.3f}</td>
                <td style="padding:6px;">{report['D√©faut']['f1-score']:.3f}</td>
            </tr>
            </tbody>
        </table>
        </div>
        """

        display(HTML(html))


    def graph_conf_mat(self, model_name: str, threshold: float = 0.5) -> None:
        """
        Affiche la matrice de confusion du mod√®le sp√©cifi√© √† un seuil donn√©.

        Parameters
        ----------
        model_name : str
            Le nom du mod√®le tel qu‚Äôil figure dans self.proba_dict.
        threshold : float, default=0.5
            Seuil √† appliquer aux probabilit√©s pour obtenir les pr√©dictions binaires.
        """
        if model_name not in self.proba_dict:
            print(f"‚ùå Mod√®le '{model_name}' non trouv√© dans les probabilit√©s.")
            return

        y_prob = self.proba_dict[model_name]
        y_pred = (y_prob >= threshold).astype(int)
        conf_mat = confusion_matrix(self.y_test, y_pred)

        plt.figure(figsize=(5.5, 4.5))
        sns.heatmap(
            conf_mat,
            annot=True,
            fmt="d",
            cmap="Blues",
            cbar=False,
            annot_kws={"size": 12},
            xticklabels=["Pr√©vu: Non d√©faut", "Pr√©vu: D√©faut"],
            yticklabels=["R√©el: Non d√©faut", "R√©el: D√©faut"]
        )

        plt.title(f"Matrice de confusion - {model_name}", fontsize=14, weight='bold')
        plt.xlabel("Pr√©diction", fontsize=12)
        plt.ylabel("R√©alit√©", fontsize=12)
        plt.xticks(fontsize=11)
        plt.yticks(fontsize=11)
        plt.tight_layout()
        plt.show()


    def get_metrics_by_threshold(
        self,
        model_name: str,
        list_metrics: List[str] = ["precision", "recall", "f1"]
    ) -> Optional[pd.DataFrame]:
        """
        Calcule les valeurs des m√©triques choisies (precision, recall, f1)
        pour une s√©rie de seuils appliqu√©s √† un mod√®le donn√©.

        Parameters
        ----------
        model_name : str
            Nom du mod√®le √† analyser (doit exister dans self.proba_dict).
        list_metrics : list of str
            M√©triques √† inclure dans le tableau : 'precision', 'recall', 'f1'.

        Returns
        -------
        pd.DataFrame or None :
            DataFrame avec les colonnes : 'threshold', 'precision', 'recall', 'f1'.
        """
        if model_name not in self.proba_dict:
            print(f"‚ùå Mod√®le '{model_name}' non trouv√© dans les probabilit√©s.")
            return None

        y_prob = self.proba_dict[model_name]
        thresholds = np.linspace(0.01, 0.99, 100)
        data = {"threshold": thresholds}

        for metric in list_metrics:
            if metric == "precision":
                data["precision"] = [precision_score(self.y_test, (y_prob >= t).astype(int), zero_division=0) for t in thresholds]
            elif metric == "recall":
                data["recall"] = [recall_score(self.y_test, (y_prob >= t).astype(int), zero_division=0) for t in thresholds]
            elif metric == "f1":
                data["f1"] = [f1_score(self.y_test, (y_prob >= t).astype(int), zero_division=0) for t in thresholds]
            else:
                print(f"‚ö†Ô∏è M√©trique non reconnue : {metric}. Ignor√©e.")

        return pd.DataFrame(data)


    def plot_metrics_by_threshold(
        self,
        model_name: str,
        list_metrics: List[str] = ["precision", "recall", "f1"]
    ) -> None:
        """
        Affiche un graphique interactif Plotly des courbes pr√©cision / rappel / F1-score
        en fonction du seuil de d√©cision pour un mod√®le donn√©.

        Parameters
        ----------
        model_name : str
            Nom du mod√®le √† analyser (doit exister dans self.proba_dict).
        list_metrics : list of str
            M√©triques √† afficher : parmi 'precision', 'recall', 'f1'.
        """
        # üîç R√©cup√©rer les m√©triques
        df = self.get_metrics_by_threshold(model_name, list_metrics=list_metrics)
        if df is None:
            return

        fig = go.Figure()

        for metric in list_metrics:
            if metric not in df.columns:
                continue

            fig.add_trace(go.Scatter(
                x=df["threshold"],
                y=df[metric],
                mode="lines+markers",
                name=metric.capitalize(),
                hovertemplate=f"Seuil = %{{x:.2f}}<br>{metric.capitalize()} = %{{y:.3f}}<extra></extra>"
            ))

        fig.update_layout(
            title=f"üìà {model_name} ‚Äî √âvolution des m√©triques selon le seuil",
            xaxis_title="Seuil de score",
            yaxis_title="Valeur de la m√©trique",
            xaxis=dict(tickformat=".2f"),
            yaxis=dict(range=[0, 1.05]),
            legend=dict(title="M√©triques"),
            template="plotly_white",
            hovermode="x unified",
            height=500,
            width=850
        )

        fig.show()


    def generate_model_report(
        self,
        n_iter: int = 50,
        cv_folds: int = 5,
        scoring: str = "f1",
    ) -> None:
        """
        G√©n√®re un rapport complet automatis√©

        Parameters
        ----------
        n_iter : int
            Nombre d'it√©rations pour l'optimisation Bayesienne.
        cv_folds : int
            Nombre de folds pour la validation crois√©e.
        scoring : str
            M√©trique utilis√©e pour la comparaison (par d√©faut "f1").
        """
        print("üîß √âtape 1 : Optimisation Bay√©sienne des mod√®les...\n")
        df_scores = self.bayes_optimize_models(n_iter=n_iter, cv_folds=cv_folds, scoring=scoring)
        display(df_scores)

        print("\nüìä √âtape 2 : Pr√©diction des probabilit√©s...")
        self.get_proba_dict()

        print("\nüìâ √âtape 3 : Affichage des courbes ROC & Pr√©cision-Rappel")
        self.plot_roc_curve_interactive()
        self.plot_precision_recall_curve_interactive()

        print("\nüéØ √âtape 4 : Optimisation du F1-score par mod√®le")
        f1_results = self.best_f1_by_model()

        # Meilleur mod√®le selon F1
        best_model = max(f1_results["f1_scores"], key=f1_results["f1_scores"].get)
        best_f1 = f1_results["f1_scores"][best_model]
        best_thresh = f1_results["opti_thresholds"][best_model]

        print(f"\n‚úÖ √âtape 5 : √âvaluation du meilleur mod√®le : {best_model}")
        print(f"   ‚Üí F1-score max = {best_f1:.4f} atteint pour un seuil = {best_thresh:.3f}\n")

        self.evaluate_model(model_name=best_model, threshold=best_thresh)
        self.graph_conf_mat(model_name=best_model, threshold=best_thresh)
        self.plot_metrics_by_threshold(model_name=best_model)
